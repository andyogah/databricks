{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dbutils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a7bc71578996>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mdbutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'file:{file_location}'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34mf'dbfs:{file_location}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'New CSV file created at dbfs:{file_location}. Contents:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dbutils' is not defined"
     ]
    }
   ],
   "source": [
    "# https://docs.databricks.com/ingestion/auto-loader/tutorial.html\n",
    "\n",
    " \n",
    "\n",
    "import csv\n",
    "import uuid\n",
    "import random\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "count = 0\n",
    "path = \"/tmp/generated_raw_csv_data\"\n",
    "Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "while True:\n",
    "\n",
    "    row_list = [ [\"id\", \"x_axis\", \"y_axis\"],\n",
    "                [uuid.uuid4(), random.randint(-100, 100), random.randint(-100, 100)],\n",
    "                [uuid.uuid4(), random.randint(-100, 100), random.randint(-100, 100)],\n",
    "                [uuid.uuid4(), random.randint(-100, 100), random.randint(-100, 100)]\n",
    "                ]\n",
    "\n",
    "    file_location = f'{path}/file_{count}.csv' \n",
    "\n",
    "    with open(file_location, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(row_list)\n",
    "        file.close() \n",
    "\n",
    "    count += 1\n",
    "    dbutils.fs.mv(f'file:{file_location}', f'dbfs:{file_location}')\n",
    "    time.sleep(30)\n",
    "    print(f'New CSV file created at dbfs:{file_location}. Contents:') \n",
    "\n",
    "    with open(f'/dbfs{file_location}', 'r') as file:\n",
    "        reader = csv.reader(file, delimiter=' ')\n",
    "        for row in reader:\n",
    "            print(', '.join(row))\n",
    "        file.close()\n",
    "\n",
    " \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "This code defines in your workspace the paths to the raw data and the target Delta table, the path to the tableâ€™s schema, and the path to the location where Auto Loader writes checkpoint file information in the Delta Lake transaction log. Checkpoints enable Auto Loader to process only new incoming data and to skip over any existing data that has already been processed.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "raw_data_location = \"dbfs:/tmp/generated_raw_csv_data\"\n",
    "target_delta_table_location = \"dbfs:/tmp/table/coordinates\"\n",
    "schema_location = \"dbfs:/tmp/auto_loader/schema\"\n",
    "checkpoint_location = \"dbfs:/tmp/auto_loader/checkpoint\" \n",
    "\n",
    "stream = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\\n",
    "    .option(\"cloudFiles.format\", \"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location) \\\n",
    "    .load(raw_data_location)\n",
    "\n",
    "display(stream)\n",
    "\n",
    "stream.writeStream \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "    .start(target_delta_table_location)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
